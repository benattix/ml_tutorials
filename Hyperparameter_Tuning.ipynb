{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "1. Setup\n",
    "1. Use Case Background\n",
    "1. Hyperparameter (HP) Definition\n",
    "1. Decision Tree Recap\n",
    "1. Data Preparation\n",
    "1. Description of HPs Decision trees (i.e. what the HPs do, how they affect the model, and tradeoffs) \n",
    "1. Build simple decision tree model with no tuning\n",
    "1. Tuning Methods\n",
    "    + Grid Search\n",
    "    + Random Search\n",
    "    + Bayesian Search\n",
    "1. Tuning with Random Forests (Optional)\n",
    "1. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Perform the following steps using a terminal:\n",
    "\n",
    "- Clone the repository: `git clone https://github.com/benattix/ml_tutorials.git`\n",
    "\n",
    "- Change directory: `cd ml_tutorials`\n",
    "\n",
    "- Create new conda environment with .yml file: `conda env create -f environment.yml`\n",
    "\n",
    "- Activate new environment `source activate hptuning`\n",
    "\n",
    "- Launch a Jupyter notebook: `jupyter notebook`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "We have a data set of beers and certain characteristics about the beer. Our goal is to determine for a given beer, how bitter it will be. Some people really enjoy bitter beers while other people hate them. By predicting how bitter a beer will be we can help people better select beers that match their tastes.\n",
    "\n",
    "The data set contains the following variables:\n",
    "+ abv - Alcohol by volume\n",
    "+ ibu - International Bitterness Units. This is a gauge of beer's bitterness\n",
    "+ id - unique identifier of each beer\n",
    "+ name - name of the beer\n",
    "+ style - style of the beer\n",
    "+ brewery_id - id of the brewery where the beer was made\n",
    "+ ounces - number of fluid ounces that the beer is sold in\n",
    "\n",
    "For our task, we will try to predict the `ibu`, since that is a measure of the bitterness.\n",
    "\n",
    "The dataset comes from [Kaggle](https://www.kaggle.com/nickhould/craft-cans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Definition\n",
    "A hyperparameter is a parameter whose value is set before the learning process begins. Parameters, on the other hand, are calculated during training (ex. linear regression coefficients)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Decision Tree Recap\n",
    "\n",
    "In Decision Trees, we start from the root of the tree. We compare the values of the root attribute with the record’s attribute. On the basis of comparison, we follow the branch corresponding to that value and jump to the next node.\n",
    "\n",
    "<img src=\"images/decision_tree.jpg\" width=500>\n",
    "\n",
    "For more detail on how decision trees work, see [Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/beers.csv', index_col=0)\n",
    "df.sort_values(by='id', inplace=True)\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are some null values. Let's check for nulls across each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Since we are trying to predict `ibu`, we will drop any rows where it is null. We cannot measure our accuracy if we do not know the true value that we're trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with null 'ibu'\n",
    "df = df.dropna(subset=['ibu'], axis=0).reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the other columns with missing values, we will fill them. Since `abv` is continuous, we will fill with the median and since `style` is categorical, we will create a new category which denotes that we do not know the style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abv'].fillna(df.abv.mean(), inplace=True)\n",
    "df['style'].fillna('NA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop id columns\n",
    "df.drop(['id', 'brewery_id'], axis=1, inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to handle our categorical variables (name and style). To do this, let's use sci-kit learn's CountVectorizer, which converts a collection of text to a matrix of token (aka word) counts. In other words, it will create a new column for each individual token, and count how many times each token appeared in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=5, binary=True)\n",
    "vec_matrix = vectorizer.fit_transform(df['name'])\n",
    "df_name = pd.DataFrame(vec_matrix.todense(), columns=['name_' + i for i in vectorizer.get_feature_names()])\n",
    "\n",
    "vec2 = CountVectorizer(min_df=5, binary=True) \n",
    "vec2_matrix = vec2.fit_transform(df['style'])\n",
    "df_style = pd.DataFrame(vec2_matrix.todense(), columns=['style_' + i for i in vec2.get_feature_names()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df.drop(['name', 'style'], axis=1)\\\n",
    "             .join(df_name)\\\n",
    "             .join(df_style)\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "X = df_final.drop(['ibu'], axis=1)\n",
    "y = df_final['ibu']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model Without Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(random_state=3)\n",
    "dt.fit(X_train, y_train)\n",
    "print('R-squared - test set: %3.3f' % dt.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Hyperparameters\n",
    "## Decision Tree\n",
    "Below are some of the hyperparameters that can be adjusted for a decision tree model. For more hyperparameters, see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html).\n",
    "+ `max_depth` : The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "+ `max_features` : The number of features to consider when looking for the best split:\n",
    "    + If int, then consider max_features features at each split.\n",
    "    + If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\n",
    "    + If “auto”, then max_features=n_features.\n",
    "    + If “sqrt”, then max_features=sqrt(n_features).\n",
    "    + If “log2”, then max_features=log2(n_features).\n",
    "    + If None, then max_features=n_features.\n",
    "+ `min_samples_leaf` : The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
    "    + If int, then consider min_samples_leaf as the minimum number.\n",
    "    + If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "+ `min_impurity_decrease` : A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "\n",
    "    The weighted impurity decrease equation is the following:\n",
    "\n",
    "    __N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity)__\n",
    "    \n",
    "    where N is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child.\n",
    "\n",
    "    N, N_t, N_t_R and N_t_L all refer to the weighted sum, if sample_weight is passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Methods\n",
    "1. [Grid Search](#grid)\n",
    "1. [Random Search](#random)\n",
    "1. [Bayesian Search](#bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"grid\"></a>\n",
    "## Grid Search\n",
    "The first method of hyperparameter tuning that we will cover is grid search. The advantages of grid search is that, compared to manually searching the hyperparamter space, grid search allows you to search more space with less code\n",
    "and it's more practical and exhaustive.\n",
    "\n",
    "We will use sci-kit learn's implementation of grid search, [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). We pass the GridSearchCV object a model and the values of the hyperparamter space for it to try and it builds a new model for each hyperparamter combination. \n",
    "\n",
    "It's important when tuning hyperparamters to still use a hold out (i.e. test) set so you can understand how well your model generalizes to data it did not see during training.\n",
    "\n",
    "<img src=\"images/grid_search.gif\" width=500>\n",
    "\n",
    "Souce: [SigOpt](https://sigopt.com/blog/common-problems-in-hyperparameter-optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'max_depth': list(range(2, 15)),\n",
    "              'min_samples_leaf': list(range(1,15,2)),\n",
    "              'min_impurity_decrease': [0.00005,0.0001,0.0005,0.001,0.005,0.01,0.1]\n",
    "              } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=10, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters:', grid.best_params_)\n",
    "print('Best cross-validation R-squared: %3.3f' % grid.best_score_)\n",
    "print('R-squared - test set: %3.3f' % grid.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"random\"></a>\n",
    "## Random Search\n",
    "When using grid search above, there was a very large hyperparameter space and it can be computationally expensive to try every single combination. \n",
    "\n",
    "With random search, we only try a subset of the combinations to avoid some unnecessary computation. Since we're only trying a subset, it's possible that the optimal hyperparameter combination will not be chosen and as a result, we could end up with slightly worse performance. \n",
    "\n",
    "\n",
    "We will use sci-kit learn's implementation of random search, [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). RandomizedSearchCV's `n_iter` parameter determines how many hyperparamter combinations we try. With the `n_iter` parameter, we can drastically reduce how much of the hyperparameter space that we explore. \n",
    "\n",
    "With RandomizedSearchCV, we can pass hyperparameters in 2 different ways:\n",
    "1. With a dictionary of values just as we did above (for GridSearchCV)\n",
    "1. With a statistical distribution. When we pass a distribution, then for each iteration the algorithm will randomly sample a value from that distrubition. Using distrubitions introduces more variation and allows for the possibility of finding an optimal hyperparameter in an area that you would not have chosen to guess.\n",
    "\n",
    "<img src=\"images/random_search.gif\" width=500>\n",
    "\n",
    "Source: [SigOpt](https://sigopt.com/blog/common-problems-in-hyperparameter-optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use the same predetermined values that we used for GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomsearch = RandomizedSearchCV(DecisionTreeRegressor(), \n",
    "                                  param_grid, \n",
    "                                  cv=10, \n",
    "                                  n_iter=30, \n",
    "                                  random_state=12,\n",
    "                                  verbose=1)\n",
    "randomsearch.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters:', randomsearch.best_params_)\n",
    "print('Best cross-validation R-squared: %3.3f' % randomsearch.best_score_)\n",
    "print('R-squared - test set: %3.3f' % randomsearch.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try RandomizedSearchCV using distributions instead of fixed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_dist = {'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                   'max_depth': stats.randint(1,10),\n",
    "                   'min_samples_leaf': stats.randint(1,15),\n",
    "                   'min_impurity_decrease': stats.uniform(1e-4, 1e-1)\n",
    "                   } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomsearch2 = RandomizedSearchCV(DecisionTreeRegressor(), \n",
    "                                   param_grid_dist, \n",
    "                                   cv=10, \n",
    "                                   n_iter=30, \n",
    "                                   random_state=12)\n",
    "randomsearch2.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters:', randomsearch2.best_params_)\n",
    "print('Best cross-validation R-squared: %3.3f' % randomsearch2.best_score_)\n",
    "print('R-squared - test set: %3.3f' % randomsearch2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomsearch2.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, RandomizedSearchCV chooses values from the distrubutions we provided. This allows us to search hyperparameter values that we might not have thought to use. This can lead to either better or worse results than GridSearchCV, depending on randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bayes\"></a>\n",
    "## Bayesian Search\n",
    "Instead of randomly searching the hyperparameter space, we can search it more efficiently. With the previous 2 methods, we experimented with different hyperparameter values and these experiments were independent of each other. Since they were independent, they were not able to use the information from one experiment to improve the next experiment.\n",
    "\n",
    "Bayesian search addresses this issue. With bayesian search, experiments are run sequentially and information from each of the previous runs are used to determine the next set of hyperparameters to try. Bayesian search uses concepts of exploration and exploitation. First, it explores the hyperparameter space by choosing some initial hyperparameter values. Then once it has some understanding of the hyperparameter space, it tries to \"exploit\" it by focusing on areas of the hyperparameter space that give the best model performance and trying slight variations of values that it already knows are good.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/bayesian_search.gif\" width=500>\n",
    "\n",
    "Souce: [SigOpt](https://sigopt.com/blog/common-problems-in-hyperparameter-optimization)\n",
    "\n",
    "Additional Resources:\n",
    "+ https://github.com/fmfn/BayesianOptimization\n",
    "+ https://github.com/hyperopt/hyperopt\n",
    "+ https://scikit-optimize.github.io/stable/auto_examples/sklearn-gridsearchcv-replacement.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth': (2, 15),\n",
    "        'min_samples_leaf': (2, 15),\n",
    "        'min_impurity_decrease': (0.0, 0.5, 'uniform'),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes = BayesSearchCV(DecisionTreeRegressor(),\n",
    "                      grid,\n",
    "                      n_iter=40,\n",
    "                      cv=10\n",
    "                     )\n",
    "\n",
    "bayes.fit(X_train, y_train)\n",
    "\n",
    "print('The best parameters are:', bayes.best_params_)\n",
    "print('The best model cross-validation R-Squared is %3.3f' % bayes.best_score_)\n",
    "print('R-squared - test set: %3.3f' % bayes.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning on Random Forest (Optional)\n",
    "Now that we have shown how to perform hyperparameter tuning on a decision tree model, try to tune a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                 'max_depth': [],   # set the max depth values to test\n",
    "                 'n_estimators': [] # set the max depth values to test\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(RandomForestRegressor(), param_grid_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_dist = {'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                   'max_depth': , # set a distribution here\n",
    "                   'n_estimators': # set a distribution here\n",
    "                   } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomsearch3 = RandomizedSearchCV(RandomForestRegressor(), \n",
    "                                   param_grid_dist, \n",
    "                                   cv=10, \n",
    "                                   n_iter=30, \n",
    "                                   random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_grid = {\n",
    "              'max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'max_depth': (), # set min and max values here\n",
    "              'n_estimators'(): # set min and max values here\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes = BayesSearchCV(RandomForestRegressor(),\n",
    "                      bayes_grid,\n",
    "                      n_iter=40,\n",
    "                      cv=10\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "Many cloud platforms can perform automated hyperparameter tuning for you. By using the cloud for tuning, you can parallelize (train multiple models at once) and use more powerful machines.\n",
    "+ AWS SageMaker\n",
    "    + https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html\n",
    "    + https://sagemaker.readthedocs.io/en/stable/tuner.html\n",
    "+ Azure\n",
    "    + https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters\n",
    "+ GCP\n",
    "    + https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
